model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"  # Or another suitable base model
project = "sanskrit-morph-rl"

[train]
micro_bs = 4
reshard_after_forward = true
# ac_ckpt = true # If using activation checkpointing

[optim]
batch_size = 128
warmup_steps = 10
total_steps = 1000000
step_per_rollout = 4 # Number of training steps per data rollout from inference

[optim.optim]
lr = 5e-7 # Learning rate, adjust as needed
# weight_decay = 0.01

[data]
path = "sanskrit_experience_data" # Should match output_path from inference config
seq_length = 512 # Should match max_model_len from inference config
# num_workers = 1 # Number of data loader workers

[ckpt]
rollout_path = "sanskrit_rollout_ckpt" # Directory where train.py saves model checkpoints for inference to pick up
clean_rollout_path = true
# path = "sanskrit_training_ckpt" # Directory for saving full training state (optimizer, scheduler, etc.)
# interval = 100 # How often to save full training state

# Optional: KL penalty, entropy loss coefficient
# kl_coef = 0.1
# entropy_loss_coeff = 0.01 