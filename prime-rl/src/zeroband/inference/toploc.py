from concurrent.futures import Future, ThreadPoolExecutor, wait
from functools import partial
from typing import TYPE_CHECKING

import torch
import torch.nn as nn
from toploc import build_proofs_bytes
from torch.utils.hooks import RemovableHandle

if TYPE_CHECKING:
from vllm import LLM
from vllm.model_executor import SamplingMetadata

# Attempt to import vllm components, provide dummies if import fails
VLLM_AVAILABLE = False
try:
from vllm.model_executor.layers.logits_processor import _prune_hidden_states
    if not TYPE_CHECKING: # Avoid redefining if already imported under TYPE_CHECKING
        from vllm import LLM 
        from vllm.model_executor import SamplingMetadata
    VLLM_AVAILABLE = True
except ImportError:
    # Define dummy/placeholder versions if vllm is not available
    # This allows the rest of the file to be parsed without error,
    # but functionalities relying on these might be disabled or limited.
    _prune_hidden_states = lambda x, y: x # No-op
    LLM = type('LLM', (object,), {}) # Dummy LLM class
    SamplingMetadata = type('SamplingMetadata', (object,), {}) # Dummy SamplingMetadata class
    # Add a log or warning here if needed, e.g.,
    # import logging
    # logging.getLogger(__name__).warning("vllm not found, TopLoc functionality will be limited.")


class TopLocCache:
    """A cache implementation for managing sequence data and generating proofs.

    This class provides functionality to store sequence data in a tensor cache and
    asynchronously generate proofs using a thread pool executor when the sequence reaches max_len.

    It allows us to generate the proof as the same time as we are generating tokens

    Args:
        max_seqs (int): Maximum number of sequences that can be stored in the cache
        hidden_size (int): Size of the hidden dimension for each sequence element
        max_len (int): Maximum length of the sequences that can be stored in the cache. Defaults to 32.
        device (torch.device | None): Device to store the cache tensor on.
            If None, the device of the first sequence will be used.
            Defaults to None.
        disable (bool): If True, disable the TOPLOC cache. Defaults to False.
    """

    def __init__(self, max_seqs: int, hidden_size: int, max_len: int = 32, device: torch.device | None = None, disable: bool = False):
        self.max_seqs = max_seqs
        self.max_len = max_len
        self.hidden_size = hidden_size
        self.device = device
        self.disable = disable

        self._cache: torch.Tensor | None = None
        self.proofs: dict[int, list[bytes]] = {}

        if not disable:
            self._executor = ThreadPoolExecutor(max_workers=8)
            self._proof_futures: dict[int, Future] = {}
        else:
            self._executor = None
            self._proof_futures = {}
            self.proofs = {
                i: [b"hello I am a proof generated by jack toploc. The world is beautiful when the rewards go up"] for i in range(max_seqs)
            }

    def _init_cache(self, device: torch.device, dtype: torch.dtype):
        """Initialize the cache tensor and related tracking structures.

        Args:
            device (torch.device): Device to store the cache tensor on
            dtype (torch.dtype): Data type for the cache tensor
        """
        if self.disable:
            return
        self._cache = torch.empty(self.max_seqs, self.max_len, self.hidden_size, device=device, dtype=dtype)
        self._seq_id_2_cache_index: dict[int, int] = {}
        self._current_seq_len: list[int] = [0 for k in range(self.max_seqs)]
        # Tracks which chunk to alloc next
        self._current_cache_index: int = 0

    def reset_cache(self):
        """Reset the cache and all tracking structures to their initial state."""
        if self.disable:
            return
        self.proofs = {}
        self._seq_id_2_cache_index = {}
        self._current_seq_len = [0 for k in range(self.max_seqs)]
        self._current_cache_index = 0

    def add(self, seq_ids: list[int], values: torch.Tensor):
        """Add new sequences to the cache.

        Args:
            seq_ids (list[int]): List of sequence IDs to add
            values (torch.Tensor): Tensor containing the sequence values to add.
                                 Shape should be [len(seq_ids), hidden_size]
        """
        if self.disable:
            return
        assert len(seq_ids) == values.shape[0]
        if self._cache is None:
            self._init_cache(self.device or values.device, values.dtype)

        for i, seq_id in enumerate(seq_ids):
            if seq_id not in self._seq_id_2_cache_index:
                self._seq_id_2_cache_index[seq_id] = self._current_cache_index
                self._current_cache_index += 1
                self.proofs[seq_id] = []
                self._proof_futures[seq_id] = None
            cache_index = self._seq_id_2_cache_index[seq_id]
            if self._proof_futures[seq_id] is not None:
                wait([self._proof_futures[seq_id]])
                self._proof_futures[seq_id] = None
            self._cache[cache_index, self._current_seq_len[cache_index]].copy_(values[i], non_blocking=True)
            self._current_seq_len[cache_index] += 1
        self.maybe_generate_proofs_in_background()

    def maybe_generate_proofs_in_background(self, force_generate: bool = False):
        """Trigger background proof generation for cached sequences.

        Proofs are generated when sequences reach max_len or when forced.

        Args:
            force_generate (bool): If True, generate proofs regardless of sequence length.
                                 Defaults to False.
        """
        if self.disable:
            return

        # Wait for all pending proofs to be generated to avoid race condition before force generating
        if force_generate:
            self.wait_for_proofs()

        for seq_id, cache_index in self._seq_id_2_cache_index.items():
            if force_generate or self._current_seq_len[cache_index] == self.max_len:
                self._proof_futures[seq_id] = self._executor.submit(
                    self._generate_proof, seq_id, cache_index, self._current_seq_len[cache_index]
                )
                self._current_seq_len[cache_index] = 0

    def _generate_proof(self, seq_id: int, cache_index: int, seq_len: int) -> None:
        """Generate proof for a specific sequence in the cache.

        Args:
            seq_id (int): ID of the sequence to generate proof for
            cache_index (int): Index of the sequence in the cache tensor
            seq_len (int): Length of the sequence to process
        """
        proof = build_proofs_bytes(self._cache[cache_index, :seq_len], decode_batching_size=self.max_len, topk=128, skip_prefill=True)[0]
        self.proofs[seq_id].append(proof)

    def wait_for_proofs(self):
        """Wait for all pending proof generation tasks to complete."""
        if self.disable:
            return

        wait(list(i for i in self._proof_futures.values() if i is not None))


def toploc_cache_hook(_, inputs: tuple, toploc_cache: TopLocCache):
    """
    Pre-hook to get final hidden states of a model forward pass and add it to
    the TOPLOC cache based on sampling metadata.


    Args:
        _ (): Unused argument, required for compatibility with hook function signature.
        input (tuple): A tuple containing the input data. The first element is expected to be the hidden states of the model, and the second element is expected to be the sampling metadata.
        toploc_cache (TopLocCache): The TopLocCache instance to which the processed data will be added.
    """
    # Get hidden states and sampling metadata from inputs
    hidden_states, sampling_metadata = inputs[1], inputs[2]
    assert isinstance(hidden_states, torch.Tensor)
    assert isinstance(sampling_metadata, SamplingMetadata)

    # This check is true only for prefills
    if max(sampling_metadata.selected_token_indices) > len(sampling_metadata.seq_groups):
        return

    # This pruning is required when cuda graph padding is enabled.
    hidden_states = _prune_hidden_states(hidden_states, sampling_metadata)
    if len(sampling_metadata.seq_groups) != hidden_states.shape[0]:
        raise ValueError(f"Lengths dont match: {len(sampling_metadata.seq_groups)} {hidden_states.shape}")

    # Get seq_ids from seq_groups
    seq_ids = [seq_group.seq_ids[0] for seq_group in sampling_metadata.seq_groups]

    # Add hidden states to TOPLOC cache with corresponding seq_ids
    toploc_cache.add(seq_ids, hidden_states)


def setup_toploc_cache(llm: LLM, disable: bool = False, **toploc_kwargs) -> tuple[TopLocCache, RemovableHandle | None]:
    """Initializes the TOPLOC cache and register a hook to dynamically populate the cache during inference"""
    # Initialize the cache
    # Ensure that toploc_cache is disabled if vLLM is not available and disable is not already True
    toploc_cache = TopLocCache(disable=disable or not VLLM_AVAILABLE, **toploc_kwargs)

    # Register hook to add hidden states to TOPLOC cache
    # This part requires a real llm object, so skip if not VLLM_AVAILABLE or if disable is True
    handle: RemovableHandle | None = None
    if VLLM_AVAILABLE and not (disable or not VLLM_AVAILABLE): # Simplified: if VLLM_AVAILABLE and not disable:
        # Ensure llm is a real LLM instance if VLLM_AVAILABLE
        # This check might be too strict if llm can be a dummy type from elsewhere.
        # For now, assume llm is expected to be a vLLM LLM object if VLLM_AVAILABLE.
        if hasattr(llm, 'llm_engine') and llm.llm_engine: # Check if llm is a valid vLLM object
            logits_processor: nn.Module = llm.llm_engine.model_executor.driver_worker.model_runner.model.logits_processor
        handle = logits_processor.register_forward_pre_hook(partial(toploc_cache_hook, toploc_cache=toploc_cache))
        else:
            # Optionally log a warning if llm object is not as expected when VLLM is available
            # import logging
            # logging.getLogger(__name__).warning("vLLM is available, but llm object is not a valid vLLM instance. TopLoc hook not registered.")
            pass # llm might be a dummy, or this path is not expected in dry run

    return toploc_cache, handle
