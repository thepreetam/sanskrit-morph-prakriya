import pytest
import torch
from toploc import verify_proofs_bytes
from vllm import SamplingParams, TokensPrompt

from zeroband.inference.toploc import TopLocCache, setup_toploc_cache

BYTES_PER_PROOF = 258


def test_toploc_disable():
    toploc_cache = TopLocCache(disable=True, max_seqs=1, max_len=64, hidden_size=1024)
    assert toploc_cache.disable is True
    assert toploc_cache.proofs == {0: [b"hello I am a proof generated by jack toploc. The world is beautiful when the rewards go up"]}


@pytest.mark.parametrize("batch_size,max_seqs", [(1, 1), (8, 64), (64, 64)])
@pytest.mark.parametrize("num_output_tokens", [16, 32, 64])
def test_toploc_cache(batch_size: int, max_seqs: int, num_output_tokens: int):
    # Initialize TOPLOC
    hidden_size = 1024
    max_len = 32
    toploc_cache = TopLocCache(max_seqs=max_seqs, max_len=max_len, hidden_size=hidden_size)
    assert toploc_cache.disable is False
    assert toploc_cache.proofs == {}

    # Add sequences in batches
    for _ in range(num_output_tokens):
        for start_idx in range(0, max_seqs, batch_size):
            seq_ids = list(range(start_idx, start_idx + batch_size))
            hidden_states = torch.randn(len(seq_ids), hidden_size, dtype=torch.bfloat16)
            toploc_cache.add(seq_ids, hidden_states)

    # Should have allocated proofs for all sequences
    assert len(toploc_cache.proofs) == max_seqs

    # Generate proofs for all sequences
    toploc_cache.maybe_generate_proofs_in_background(force_generate=True)
    toploc_cache.wait_for_proofs()

    # Check TOPLOC cached proofs
    assert len(toploc_cache.proofs) == max_seqs
    expected_num_proofs = (num_output_tokens + max_len - 1) // max_len  # ceil(num_output_tokens / max_len)
    assert all([len(proof) == expected_num_proofs for proof in toploc_cache.proofs.values()])

    # Check concatenated byte string proofs
    proofs = [b"".join(proofs) for proofs in sorted(toploc_cache.proofs.values())]
    assert len(proofs) == max_seqs
    assert all([type(proof) is bytes for proof in proofs])
    assert all([len(proof) % BYTES_PER_PROOF == 0 for proof in proofs])
    assert all([len(proof) // BYTES_PER_PROOF == expected_num_proofs for proof in proofs])


def test_toploc_cache_exact_proof():
    # Initialize TOPLOC
    max_seqs, hidden_size, max_len = 1, 1024, 32
    toploc_cache = TopLocCache(max_seqs=max_seqs, max_len=max_len, hidden_size=hidden_size)
    assert toploc_cache.disable is False
    assert toploc_cache.proofs == {}

    # Seed everything
    torch.manual_seed(69)
    torch.cuda.manual_seed(69)

    # Add sequences in batches
    num_output_tokens = 32
    for _ in range(num_output_tokens):
        seq_ids = [0]
        hidden_states = torch.randn(len(seq_ids), hidden_size, dtype=torch.bfloat16)
        toploc_cache.add(seq_ids, hidden_states)

    # Should have allocated proofs for all sequences
    assert len(toploc_cache.proofs) == max_seqs

    # Generate proofs for all sequences
    toploc_cache.maybe_generate_proofs_in_background(force_generate=True)
    toploc_cache.wait_for_proofs()

    # Check TOPLOC cached proofs
    proof = list(toploc_cache.proofs.values())[0][0]
    assert (
        proof
        == b"\xff\xd9\xee\x88\x862\xeb\xe2\xfd\x8ch\xc2o\x95\xef\xdd{\xfdI\x03\xd0\xb1\xa3mZ \xd4\xb0\x07m\xc2\x17\xaa\xeb)\x03\x9fv\xdb\xd5\xb9Y\xef\x871\xf2\x98\xc7\xe5\x03@4l\xfb\x17\x1b\x1fN]\x85\x9cs}\x95\xf0$\x0c^\xe4\x15\xa5\x0e\x90J\xf8;\x1e\xa1&\xdb\x8aM\xf10:\xdfX\x8a\x8e\xb1'\xe3\xe0\xb5S\xaf!38\xda`\n\x86\x04\xd5\xd0lH\xe1\xa9\x9c\xdbe\x99@\x1an\xc1\xcb\xf7\x85\x8d\xea\x99\x90\xeemM\x06\xc0\x14\xc0w\x9fs\x9c\x9c\x94D\x90\xc5\xc6\xa8?6\xc7\xe4\xde\xc8A\xa4\x19@Jd\xa3\tc\x99\xe1@(\x06\xd3 \xcc\xe4\x01\xb2\xf5\xf0\x17\r$#\xcb\xf2\xfa\x02O\xc8-\xcf\x90\xb9\xdd\x95\xd7\xde\xddZ\xb4\xab\xec\xa6?\xac0x/{,\x8d9\xf6\x1ab4W\x0f\x86\xdd\xf7\xb6V\xb8\x11\xba\xa9N\x92\xd1\xc3\x7fa\x1a\x0e2\x10\xfeq.\x14Nb^5\x8e\x9fW\xeb\x9e\xf5!6\xab\xcf\x98G\xd8\x14\x1a\x85k\x00\xdb\xa1\xd1"
    )


@pytest.mark.parametrize("max_seqs", [1, 8, 64])
@pytest.mark.parametrize("num_output_tokens", [16, 32, 64])
@pytest.mark.slow
@pytest.mark.gpu
def test_toploc_with_hook(llm, max_seqs: int, num_output_tokens: int):
    # Setup TOPLOC
    model = llm.llm_engine.model_executor.driver_worker.model_runner.model
    hidden_size = model.config.hidden_size
    max_len = 32
    toploc_cache, hook_handle = setup_toploc_cache(llm, max_seqs=max_seqs, hidden_size=hidden_size, max_len=max_len)
    assert toploc_cache.disable is False
    assert toploc_cache.proofs == {}

    # Generate sequences
    prompts = ["My name is"] * max_seqs
    sampling_params = SamplingParams(min_tokens=num_output_tokens, max_tokens=num_output_tokens, seed=69, temperature=0.0)
    generations = llm.generate(prompts, sampling_params=sampling_params)

    # Generate proofs for all sequences
    toploc_cache.maybe_generate_proofs_in_background(force_generate=True)
    toploc_cache.wait_for_proofs()

    # Check TOPLOC cached proofs
    assert len(toploc_cache.proofs) == max_seqs
    expected_num_proofs = (num_output_tokens + max_len - 1) // max_len
    assert all([len(proof) == expected_num_proofs for proof in toploc_cache.proofs.values()])

    # Check concatenated byte string proofs
    proofs = [b"".join(proofs) for proofs in sorted(toploc_cache.proofs.values())]
    assert len(proofs) == max_seqs
    assert all([type(proof) is bytes for proof in proofs])
    assert all([len(proof) % BYTES_PER_PROOF == 0 for proof in proofs])
    assert all([len(proof) // BYTES_PER_PROOF == expected_num_proofs for proof in proofs])

    # Validate proofs using pre-fill hook
    hook_handle.remove()
    full_activations = []
    hook_handle = model.logits_processor.register_forward_pre_hook(lambda _, inputs: full_activations.append(inputs[1]))
    for generation, proof in zip(generations, proofs):
        prompt_tokens = list(generation.prompt_token_ids)
        output_tokens = list(generation.outputs[0].token_ids)

        token_input = TokensPrompt(prompt_token_ids=prompt_tokens + output_tokens)
        llm.generate(token_input, sampling_params=SamplingParams(max_tokens=1))

        # Only grab non-prompt tokens (exclude BOS)
        decode_activations = torch.concat(full_activations, dim=0)[len(prompt_tokens) :]
        full_activations.clear()
        assert decode_activations.shape == (num_output_tokens, hidden_size)

        # Verify proofs
        chunked_proofs = [proof[i : i + BYTES_PER_PROOF] for i in range(0, len(proof), BYTES_PER_PROOF)]
        results = verify_proofs_bytes(decode_activations, chunked_proofs, decode_batching_size=max_len, topk=128, skip_prefill=True)
        assert all([result.exp_mismatches < 60 for result in results])

    # Remove hook
    hook_handle.remove()
